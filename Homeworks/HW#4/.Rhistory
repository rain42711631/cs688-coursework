if(input$actb>=0){
withProgress(
message = 'Calculation in progress',
value = 0, {
# Number of times we'll go through the loop
n <- 15
for (i in 1:n) {
incProgress(1/n, detail = paste("This may take a while", i))
# Pause for 0.1 seconds to simulate a long computation.
Sys.sleep(0.1)
}
}
)
}
entity2<- input$entity2
})
# wordcloud(names(query1.dtm.counts), query1.dtm.counts, min.freq = 5, colors=brewer.pal(8, "Dark2"))
# wordcloud(names(query2.dtm.counts), query2.dtm.counts, min.freq = 10, colors=brewer.pal(8, "Dark2"))
output$entity1wc <- renderText(search.term.1)
output$entity1wcplot <- renderPlot({
if(input$actb>=0){
withProgress(
message = 'Calculation in progress',
detail = 'This may take a while...',
value = 0, {
# Number of times we'll go through the loop
n <- 15
for (i in 1:n) {
incProgress(1/n)
Sys.sleep(0.1)
}
}
)
}
wordcloud(t$Terms, t$q1.freq, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
})
output$entity2wc <- renderText(input$entity2)
output$entity2wcplot <- renderPlot({
if(input$actb>=0){
withProgress(
message = 'Calculation in progress',
detail = 'This may take a while...',
value = 0, {
# Number of times we'll go through the loop
n <- 15
for (i in 1:n) {
incProgress(1/n)
Sys.sleep(0.1)
}
}
)
}
wordcloud(t$Terms, t$q2.freq, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
})
}
shinyUI <- fluidPage(
headerPanel("Page rank and content analysis"),
# Getting User Inputs
sidebarPanel(
wellPanel(
textInput("entity1", "Query 1: ", "microsoft"),
textInput ("entity2","Query 2: ", "apple"),
HTML
("<div style='font-size: 10px;font-weight: bold'> Enter the search queries that you want '#'</div>")
)  ,
wellPanel(
# sliderInput("maxTweets","Number of recent tweets to use for analysis:",min=10,max=1000,value=200,step=1), # The max can, of course, be increased
actionButton(inputId='actb',icon =icon("google"), label="Analyze!")
)
),
mainPanel(
tabsetPanel(
#Output from tab 1 ----So a box plot to see the distribution of scores of sentiments
tabPanel("Top links", HTML
("<div> These are the top links. </div>"),
tableOutput("toplinks1"),tableOutput("toplinks2"),id="test"),
tabPanel("Processed Links", tableOutput("processedlinks1"), tableOutput("processedlinks2")),
tabPanel("Wiki references", splitLayout(cellWidths = c("50%", "50%"), tableOutput("wikihrefs1"), tableOutput("wikihrefs2"))),
#Output from tab 2 - Word clouds - with some html tags
tabPanel("File types", plotOutput("filetypes")),
tabPanel("Word Clouds",h2(textOutput("entity1wc")),plotOutput("entity1wcplot"),h2(textOutput("entity2wc")),plotOutput("entity2wcplot")),
#Output from tabs 3 and 4, map of tweets locations
tabPanel("Common wiki links", tableOutput("commonwikilinks")),
#Output from tabs 5 and 6, the raw tweets
tabPanel("Common contents Terms",tableOutput("tableentity1")),
tabPanel("Top contents",tableOutput("tableentity2")),
tabPanel("Common content Graph",plotOutput("commoncontent1")),
tabPanel("Wiki Links Network Graph",plotOutput("contentsgraph1", width = "100%"))
)
)
)
shinyApp(server = shinyServer, ui = shinyUI)
## Project temp -new working code
rm(list=ls())
library(RCurl)
library(XML)
library(url)
library(RJSONIO)
library(googleVis)
library(ggplot2)
library(reshape)
library(tm)
library(tm.plugin.webmining)
library(rvest)
library(qdap)
library(stringi)
library(stringr)
library(xml2)
library(shiny)
library(wordcloud)
library(igraph)
setwd('Documents/BU Courseworks/Fall-2016/CS688/Project/')
shinyServer <- function(input, output, session){
# Function to create a google search URL
getGoogleURL <- function(search.term, domain = '.com', quotes=TRUE) {
search.term <- gsub(' ', '%20', search.term)
if(quotes) search.term <- paste('%22', search.term, '%22', sep='')
getGoogleURL <- paste('http://www.google', domain, '/search?q=',
search.term, sep='')
}
# Function to get google top search links
getGoogleLinks <- function(google.url) {
doc <- getURL(google.url, httpheader = c("User-Agent" = "R
(2.10.0)"))
html <- htmlTreeParse(doc, useInternalNodes = TRUE, error=function
(...){})
nodes <- getNodeSet(html, "//h3[@class='r']//a")
return(sapply(nodes, function(x) x <- xmlAttrs(x)[["href"]]))
}
# Get information from google for search terms
links <- function(links1, links2, quotes){
search.url.1 <- getGoogleURL(search.term=search.term.1, quotes=quotes)
search.url.2 <- getGoogleURL(search.term=search.term.2, quotes=quotes)
# search.url.1
# search.url.2
links.1 <- getGoogleLinks(search.url.1)
links.2 <- getGoogleLinks(search.url.2)
return (list(links.1, links.2))
}
# Function to crop the url's
crop.links <- function(links){
crop.links = vector("list", length(links))
crop.links
for (ii in 1:length(links)){
crop.links[ii] <- substring(links[ii], gregexpr(pattern ='http',links[ii])[[1]][1], (gregexpr(pattern ='&sa',links[ii])[[1]][1])-1)
}
return(crop.links)
}
# Function to separate the images vs graphics
separate.files <- function(wikifiles.1, wikifiles.2){
files.1.p <- grep(paste(c("jpg","png"),collapse="|"), wikifiles.1, value=TRUE, ignore.case = TRUE)
files.1.g <- grep("svg", wikifiles.1, value=TRUE, ignore.case = TRUE)
files.1.pictures <- substring(files.1.p, 6, nchar(files.1.p))
files.1.graphics <- substring(files.1.g, 6, nchar(files.1.g))
indices.2 <- grep(paste(c("jpg","png"),collapse="|"), wikifiles.2, value = FALSE, ignore.case = TRUE)
files.2.p <- wikifiles.2[indices.2]
files.2.g <- grep("svg", wikifiles.2, value = TRUE, ignore.case = TRUE)
files.2.pictures <- substring(files.2.p, 6, nchar(files.2.p))
files.2.graphics <- substring(files.2.g, 6, nchar(files.2.g))
return (list(files.1.pictures, files.1.graphics, files.2.pictures, files.2.graphics))
}
# search.term.1 <- "web mining"
# search.term.2 <- "text mining"
# search.term.1 <- "yahoo wiki"
# search.term.2 <- "microsoft wiki"
search.term.1 <- "microsoft "
search.term.2 <- "apple"
quotes <- "FALSE"
links.12 <- links(search.term.1, search.term.2,quotes)
links.1 <- links.12[[1]]
links.2 <- links.12[[2]]
length(links.1)
length(links.2)
output$toplinks1 <- renderTable(Query1 <- unlist(links.1))
output$toplinks2 <- renderTable(Query2 <-unlist(links.2))
cropped.links.1 <- crop.links(links.1)
cropped.links.2 <- crop.links(links.2)
output$processedlinks1 <- renderTable(Query1 <- unlist(cropped.links.1))
output$processedlinks2 <- renderTable(Query2 <- unlist(cropped.links.2))
# Finding the wikipedia links
wiki.index.1 <- grep('wikipedia', cropped.links.1)
wiki.index.2 <- grep('wikipedia', cropped.links.2)
wiki.index.1[1]
wiki.index.2[1]
wiki.link.1 <- cropped.links.1[[wiki.index.1[1]]]
wiki.link.2 <- cropped.links.2[[wiki.index.2[1]]]
wiki.link.1
wiki.link.2
# # For xml content ONLY
# url <- "http://stackoverflow.com/questions/3746256/extract-links-from-webpage-using-r"
# doc <- htmlParse(url)
# typeof(doc)
# links <- xpathSApply(doc, "//a/@href")
# free(doc)
# length(links)
# links[1:10]
# For wikipedia ONLY
html.1 <- read_html(wiki.link.1)
html.hrefs.1 <- html_attr(html_nodes(html.1, "a"), "href")
length(html.hrefs.1)
head(html.hrefs.1,20)
html.2 <- read_html(wiki.link.2)
html.hrefs.2 <- html_attr(html_nodes(html.2, "a"), "href")
length(html.hrefs.2)
head(html.hrefs.2,20)
wiki.hrefs.1 <- html.hrefs.1[grep("wiki/", html.hrefs.1)]
wiki.hrefs.2 <- html.hrefs.2[grep("wiki/", html.hrefs.2)]
wiki.hrefs.2[1:50]
wiki.hrefs.1[1:50]
wiki.linkwords.1 <- (substring(wiki.hrefs.1, 7, nchar(wiki.hrefs.1)))
wiki.linkwords.1 <- gsub("[^[:alnum:],]", " ", wiki.linkwords.1)
length(wiki.linkwords.1)
# wiki.linkwords.1[1:50]
wiki.linkwords.2 <- (substring(wiki.hrefs.2, 7, nchar(wiki.hrefs.2)))
wiki.linkwords.2 <- gsub("[^[:alnum:],]", " ", wiki.linkwords.2)
length(wiki.linkwords.2)
# wiki.linkwords.2[1:50]
# Writing all link words to a file
#   fileConn<-file("linkwords1.txt")
#   writeLines(wiki.linkwords.1, fileConn)
#   close(fileConn)
#
#   fileConn<-file("linkwords2.txt")
#   writeLines(wiki.linkwords.2, fileConn)
#   close(fileConn)
#   list.files()
#
wiki.linkwords.1 <- wiki.linkwords.1[1: min(c(match("Help Authority control", wiki.linkwords.1),
match("Help Category", wiki.linkwords.1), match("Help Contents", wiki.linkwords.1)),na.rm = TRUE)-1]
length(wiki.linkwords.1)
# wiki.linkwords.1[798]
length(wiki.linkwords.2)
wiki.linkwords.2 <- wiki.linkwords.2[1: min(c(match("Help Authority control", wiki.linkwords.2),
match("Help Category", wiki.linkwords.2), match("Help Contents", wiki.linkwords.2)), na.rm = TRUE)-1]
length(wiki.linkwords.2)
# wiki.linkwords.2[712]
output$wikihrefs1 <- renderTable(data.frame( Query1 = wiki.hrefs.1[1:50], Query2 = wiki.hrefs.2[1:50]))
output$wikihrefs2 <- renderTable(data.frame( Query1 = wiki.linkwords.1[1:50], Query2 = wiki.linkwords.2[1:50]))
# Finding the same wiki links
common.wiki.links <- Reduce(intersect, list(wiki.linkwords.1, wiki.linkwords.2))
length(common.wiki.links)
output$commonwikilinks <- renderTable(CommonWikiWords <- common.wiki.links)
# Files(images) inside the wiki h-refernces
wiki.file.1 <- unique(wiki.linkwords.1[grepl("File", wiki.linkwords.1)])
wiki.file.1
wiki.file.2 <- unique(wiki.linkwords.2[grepl("File", wiki.linkwords.2)])
wiki.file.2
wiki.files <- separate.files(wiki.file.1, wiki.file.2)
sapply(wiki.files, length)
wiki.files[[1]]
data.x = melt(data.frame("JPG/PNG"=c(length(wiki.files[[1]]),length(wiki.files[[3]])),
"SVG"=c(length(wiki.files[[2]]),length(wiki.files[[4]])),
SearchTerms=c("Link-1", "Link-2")), variable_name="metric")
data.x
output$filetypes <- renderPlot({
if(input$actb>=0){
withProgress(
message = 'Calculation in progress',
detail = 'This may take a while...',
value = 0, {
# Number of times we'll go through the loop
n <- 15
for (i in 1:n) {
incProgress(1/n)
Sys.sleep(0.1)
}
}
)
}
ggplot(data.x, aes(SearchTerms, value, fill=metric)) +
geom_bar(position="dodge", stat="identity") +
ggtitle("File types of search terms")
})
# Striping the HTML contents
p.text.1 <- html.1 %>% html_nodes("p")
p.text.2 <- html.2 %>% html_nodes("p")
p.text.1 <- gsub("<.*?>", "", p.text.1)
p.text.2 <- gsub("<.*?>", "", p.text.2)
length(p.text.1)
length(p.text.2)
# q1.content <- bracketX(html.1, "angle")
# q1.content <- gsub("\\{.*?\\}", "", q1.content)
# q1.content <- gsub("\\(.*?\\)", "", q1.content)
# # q1.content
# q2.content <- bracketX(html.2, "angle")
# q2.content <- gsub("\\{.*?\\}", "", q2.content)
# q2.content <- gsub("\\(.*?\\)", "", q2.content)
# # q2.content
#
# q1.content <- str_replace_all(q1.content, "[[:punct:]]", "")
# q1.content <- gsub('[[:digit:]]+', '', q1.content)
# q1.content <- gsub("[^0-9A-Za-z///' ]", "", q1.content)
# q1.content <- gsub('\\s+', ' ',q1.content)
# # q1.content
#
# q2.content <- str_replace_all(q2.content, "[^[:alnum:]]", " ")
# q2.content <- gsub('[[:digit:]]+', '', q2.content)
# q2.content <- gsub("[^0-9A-Za-z///' ]", "", q2.content)
# q2.content <- gsub('\\s+', ' ',q2.content)
# # q2.content
# barplot(as.numeric(lapply(wiki.files, length)))
#
# matrix(as.numeric(lapply(wiki.files, length)), nr=2)
# barplot(matrix(as.numeric(lapply(wiki.files, length)), nr=2), beside=T,
#         col=c("aquamarine3","coral"), names.arg= c("Link-1", "Link-2"))
# legend("topleft", c("JPG/PNG", "SVG"), pch=15,
#        col=c("aquamarine3","coral"),
#        bty="n")
#
query1.corpus <- Corpus(VectorSource(p.text.1))
query1.corpus.temp <- tm_map(query1.corpus, removeNonASCII)
query1.corpus.temp <- tm_map(query1.corpus.temp, removeNumbers)
query1.corpus.temp <- tm_map(query1.corpus.temp, removePunctuation)
query1.corpus.temp <- tm_map(query1.corpus.temp, removeWords, stopwords("english"))
# query1.corpus.temp <- tm_map(query1.corpus.temp, stemDocument)
print(query1.corpus.temp);
query1.corpus.dtm <- DocumentTermMatrix(query1.corpus.temp)
# query1.corpus.dtm <- DocumentTermMatrix(query1.corpus)
query1.dtm.counts <- colSums(as.matrix(query1.corpus.dtm))
length(query1.dtm.counts)
query1 <- data.frame(Terms = names(query1.dtm.counts), Frequency = query1.dtm.counts)
query1 <- data.frame(Terms = query1[,1], Frequency = query1[,2])
head(query1)
query1 <- query1[order(query1$Frequency, decreasing = TRUE),]
head(query1,10)
#
query2.corpus <- Corpus(VectorSource(p.text.2))
query2.corpus.temp <- tm_map(query2.corpus, removeNonASCII)
query2.corpus.temp <- tm_map(query2.corpus.temp, removeNumbers)
query2.corpus.temp <- tm_map(query2.corpus.temp, removePunctuation)
query2.corpus.temp <- tm_map(query2.corpus.temp, removeWords, stopwords("english"))
# query2.corpus.temp <- tm_map(query2.corpus.temp, stemDocument)
# query2.corpus.dtm <- DocumentTermMatrix(query2.corpus.temp)
query2.corpus.dtm <- DocumentTermMatrix(query2.corpus)
query2.dtm.counts <- colSums(as.matrix(query2.corpus.dtm))
length(query2.dtm.counts)
query2 <- data.frame(Terms = names(query2.dtm.counts), Frequency = query2.dtm.counts)
query2 <- data.frame(Terms = query2[,1], Frequency = query2[,2])
head(query2)
query2 <- query2[order(query2$Frequency, decreasing = TRUE),]
head(query2,10)
# Finding the common words in queries' contents
common.contents <- intersect(query1$Terms, query2$Terms)
nrow(query1)
t1 <- query1[query1$Terms %in% common.contents, ]
nrow(query2)
t2 <- query2[query2$Terms %in% common.contents, ]
t <- data.frame(Terms =t1[order(t1$Terms),]$Terms, q1.freq= t1[order(t1$Terms),]$Frequency, q2.freq= t2[order(t2$Terms),]$Frequency)
head(t,20)
head(sort(t$q1.freq, decreasing = TRUE), 20)
head(sort(t$q2.freq, decreasing = TRUE), 20)
relations <- data.frame(from = c(rep(search.term.1, 50),rep(search.term.2,50)),
to = rep(t$Terms[1:50],2),
weight = c(t$q1.freq[1:50], t$q2.freq[1:50]))
g <- graph.data.frame(relations, directed=TRUE)
plot(g, edge.width=E(g)$weight,layout = layout.fruchterman.reingold,main = g$name,
vertex.label = V(g)$name, vertex.size = 20, vertex.color= V(g)$color, vertex.frame.color= "white",
vertex.label.color = "white", vertex.label.family = "sans", edge.color="black",edge.label=round(E(g)$weight, 3))
output$contentsgraph1 <- renderPlot({
if(input$actb>=0){
withProgress(
message = 'Calculation in progress',
detail = 'This may take a while...',
value = 0, {
# Number of times we'll go through the loop
n <- 15
for (i in 1:n) {
incProgress(1/n)
Sys.sleep(0.1)
}
}
)
}
plot(g, edge.width=E(g)$weight,layout = layout.fruchterman.reingold, main = g$name,
vertex.label = V(g)$name, vertex.size = 20, vertex.color= V(g)$color, vertex.frame.color= "white",
vertex.label.color = "white", vertex.label.family = "sans", edge.color="black",edge.label=round(E(g)$weight, 3))
},height = 1000, width = 1000)
output$tableentity1 <- renderTable(common.contents)
output$tableentity2 <- renderTable(t)
output$commoncontent1 <- renderPlot({
if(input$actb>=0){
withProgress(
message = 'Calculation in progress',
detail = 'This may take a while...',
value = 0, {
# Number of times we'll go through the loop
n <- 15
for (i in 1:n) {
incProgress(1/n)
Sys.sleep(0.1)
}
}
)
}
t12 <- t[t$q1.freq >20 | t$q2.freq >20, ]
dfp1 <- melt(t12)
names(dfp1)[3] <- "q2.freq"
ggplot(dfp1, aes(x = Terms, y= q2.freq, fill = variable), xlab="Age Group") +
geom_bar(stat="identity", width=.5, position = "dodge")+ ggtitle("Content comparison") +
theme(axis.text.x=element_text(angle=45, size =10))
})
entity1<-reactive({
if(input$actb>=0){
withProgress(
message = 'Calculation in progress',
value = 0, {
# Number of times we'll go through the loop
n <- 15
for (i in 1:n) {
incProgress(1/n, detail = paste("This may take a while", i))
# Pause for 0.1 seconds to simulate a long computation.
Sys.sleep(0.1)
}
}
)
}
entity1<- input$entity1
})
#entity 2
entity2<-reactive({
if(input$actb>=0){
withProgress(
message = 'Calculation in progress',
value = 0, {
# Number of times we'll go through the loop
n <- 15
for (i in 1:n) {
incProgress(1/n, detail = paste("This may take a while", i))
# Pause for 0.1 seconds to simulate a long computation.
Sys.sleep(0.1)
}
}
)
}
entity2<- input$entity2
})
# wordcloud(names(query1.dtm.counts), query1.dtm.counts, min.freq = 5, colors=brewer.pal(8, "Dark2"))
# wordcloud(names(query2.dtm.counts), query2.dtm.counts, min.freq = 10, colors=brewer.pal(8, "Dark2"))
output$entity1wc <- renderText(search.term.1)
output$entity1wcplot <- renderPlot({
if(input$actb>=0){
withProgress(
message = 'Calculation in progress',
detail = 'This may take a while...',
value = 0, {
# Number of times we'll go through the loop
n <- 15
for (i in 1:n) {
incProgress(1/n)
Sys.sleep(0.1)
}
}
)
}
wordcloud(t$Terms, t$q1.freq, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
})
output$entity2wc <- renderText(input$entity2)
output$entity2wcplot <- renderPlot({
if(input$actb>=0){
withProgress(
message = 'Calculation in progress',
detail = 'This may take a while...',
value = 0, {
# Number of times we'll go through the loop
n <- 15
for (i in 1:n) {
incProgress(1/n)
Sys.sleep(0.1)
}
}
)
}
wordcloud(t$Terms, t$q2.freq, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
})
}
shinyUI <- fluidPage(
headerPanel("Page rank and content analysis"),
# Getting User Inputs
sidebarPanel(
wellPanel(
textInput("entity1", "Query 1: ", "microsoft"),
textInput ("entity2","Query 2: ", "apple"),
HTML
("<div style='font-size: 10px;font-weight: bold'> Enter the search queries that you want '#'</div>")
)  ,
wellPanel(
# sliderInput("maxTweets","Number of recent tweets to use for analysis:",min=10,max=1000,value=200,step=1), # The max can, of course, be increased
actionButton(inputId='actb',icon =icon("google"), label="Analyze!")
)
),
mainPanel(
tabsetPanel(
#Output from tab 1 ----So a box plot to see the distribution of scores of sentiments
tabPanel("Top links", HTML
("<div> These are the top links. </div>"),
tableOutput("toplinks1"),tableOutput("toplinks2"),id="test"),
tabPanel("Processed Links", tableOutput("processedlinks1"), tableOutput("processedlinks2")),
tabPanel("Wiki references", splitLayout(cellWidths = c("50%", "50%"), tableOutput("wikihrefs1"), tableOutput("wikihrefs2"))),
#Output from tab 2 - Word clouds - with some html tags
tabPanel("File types", plotOutput("filetypes")),
tabPanel("Word Clouds",h2(textOutput("entity1wc")),plotOutput("entity1wcplot"),h2(textOutput("entity2wc")),plotOutput("entity2wcplot")),
#Output from tabs 3 and 4, map of tweets locations
tabPanel("Common wiki links", tableOutput("commonwikilinks")),
#Output from tabs 5 and 6, the raw tweets
tabPanel("Common contents Terms",tableOutput("tableentity1")),
tabPanel("Top contents",tableOutput("tableentity2")),
tabPanel("Common content Graph",plotOutput("commoncontent1")),
tabPanel("Wiki Links Network Graph",plotOutput("contentsgraph1", width = "100%"))
)
)
)
shinyApp(server = shinyServer, ui = shinyUI)
